class System:
    """
    A Python class containing the vision model and the specific classifier to interact with.
    
    Attributes
    ----------
    classifier_num : int
        The unit number of the classifier.
    layer : string
        The name of the layer where the classifier is located.
    model_name : string
        The name of the vision model.
    model : nn.Module
        The loaded PyTorch model.

    Methods
    -------
    call_classifier(image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]
        Returns the classifier confidence score for each image in the input image_list as well as the original image (encoded into a Base64 string).
    """

    def __init__(self, classifier_num: int, layer: str, model_name: str, device: str):
        """
        Initializes a classifier object by specifying its number and layer location and the vision model that the classifier belongs to.
        Parameters
        -------
        classifier_num : int
            The unit number of the classifier.
        layer : str
            The name of the layer where the classifier is located.
        model_name : str
            The name of the vision model that the classifier is part of.
        device : str
            The computational device ('cpu' or 'cuda').
        """
        self.classifier_num = classifier_num
        self.layer = layer
        self.device = torch.device(f"cuda:{device}" if torch.cuda.is_available() else "cpu")
    
    def call_classifier(self, image_list: List[torch.Tensor]) -> Tuple[List[float], List[str]]:
        """
        The function returns the classifier’s maximum confidence score (in float format) for each of the images in the list as well as the original image (encoded into a Base64 string).
        
        Parameters
        ----------
        image_list : List[torch.Tensor]
            The input image
        
        Returns
        -------
        Tuple[List[int], List[str]]
            For each image in image_list returns the confidence score of the classifier on that image, and the original image encoded into a Base64 string.

        
        Examples
        --------
        >>> # test the confidence score of the classifier for the prompt "a dog standing on the grass"
        >>> prompt = ["a dog standing on the grass"]
        >>> image = tools.text2image(prompt)
        >>> score_list, image_list = system.call_classifier(image)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        >>>
        >>> # test the confidence score of the classifier for the prompt "a dog standing on the grass" and maintain robustness to noise
        >>> prompts = ["a dog standing on the grass"]*5
        >>> images = tools.text2image(prompts)
        >>> score_list, image_list = system.call_classifier(images)
        >>> tools.display(image_list[0], f"Confidence Score: {statistics.mean(score_list)}")
        >>>
        >>> # test the confidence score of the classifier for the prompt "a landscape with a tree and river"
        >>> # for the same image but with different seasons:
        >>> prompts = ["a landscape with a tree and a river"]*3
        >>> original_images = tools.text2image(prompts)
        >>> edits = ["make it autumn","make it spring","make it winter"]
        >>> all_images, all_prompts = tools.edit_images(original_images, edits)
        >>> score_list, image_list = system.call_classifier(all_images)
        >>> for score, image, prompt in zip(score_list, image_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nConfidence Score: {score}")
        """

class Tools:
    """
    A Python class containing tools to interact with the units implemented in the system class, 
    in order to run experiments on it.

    Attributes
    ----------
    text2image_model_name : str
        The name of the text-to-image model.
    text2image_model : any
        The loaded text-to-image model.
    images_per_prompt : int
        Number of images to generate per prompt.
    path2save : str
        Path for saving output images.
    threshold : any
        Confidence score threshold for classifier analysis.
    device : torch.device
        The device (CPU/GPU) used for computations.
    experiment_log: str
        A log of all the experiments, including the code and the output from the classifier
        analysis.
    exemplars : Dict
        A dictionary containing the exemplar images for each unit.
    exemplars_scores : Dict
        A dictionary containing the confidence scores for each exemplar image.
    exemplars_thresholds : Dict
        A dictionary containing the threshold values for each unit.
    results_list : List
        A list of the results from the classifier analysis.

    Methods
    -------
    edit_images(self, base_images: List[str], editing_prompts: List[str]) -> Tuple[List[List[str]], List[str]]
        This function enables localized testing of specific hypotheses about how
        variations on the content of a single image affect classifier confidence scores.
        Gets a list of input images in Base64 encoded string format and a list of 
        corresponding editing instructions, then edits each provided image based on the 
        instructions given in the prompt using a text-based image editing model. The 
        function returns a list of images in Base64 encoded string format and list of the 
        relevant prompts. This function is very useful for testing the causality of the 
        classifier in a controlled way, or example by testing how the classifier confidence 
        score is affected by changing one aspect of the image. IMPORTANT: Do not use negative 
        terminology such as "remove ...", try to use terminology like "replace ... with ..." 
        or "change the color of ... to ...".
    text2image(prompt_list: str) -> List[str]
        Gets a list of text prompts as an input and generates an image for each
        prompt using a text to image model. The function returns a
        list of images in Base64 encoded string format.
    summarize_images(self, image_list: List[str]) -> str:    
        This function is useful to summarize the mutual visual concept that
        appears in a set of images. It gets a list of images at input and
        describes what is common to all of them.
    describe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str
        Provides impartial descriptions of images. Gets a list of images 
        and generates a textual description of the semantic content of each of them.
        The function is blind to the current hypotheses list and
        therefore provides an unbiased description of the visual content.
    display(self, *args: Union[str, Image.Image]):
        This function is your way of displaying experiment data. You must call
        this on results/variables that you wish to view in order to view them. 
    """    

    def __init__(self, path2save: str, device: str, images_per_prompt=1, text2image_model_name='sd'):
        """
        Initializes the Tools object.

        Parameters
        ----------
        path2save : str
            Path for saving output images.
        device : str
            The computational device ('cpu' or 'cuda').
        images_per_prompt : int
            Number of images to generate per prompt.
        text2image_model_name : str
            The name of the text-to-image model.
        """
        
    def edit_images(self,
                    base_images: List[str],
                    editing_prompts: List[str]) -> Tuple[List[List[str]], List[str]]:
        """
        Generates or uses provided base images, then edits each base image with a
        corresponding editing prompt. Accepts either text prompts or Base64
        encoded strings as sources for the base images.

        The function returns a list containing lists of images (original and edited,
        interleaved) in Base64 encoded string format, and a list of the relevant
        prompts (original source string and editing prompt, interleaved).

        Parameters
        ----------
        base_images : List[str]
            A list of images as Base64 encoded strings. These images are to be 
            edited by the prompts in editing_prompts.
        editing_prompts : List[str]
            A list of instructions for how to edit the base images derived from
            `base_images`. Must be the same length as `base_images`.

        Returns
        -------
        Tuple[List[List[str]], List[str]]
            - all_images: A list where elements alternate between:
                - A list of Base64 strings for the original image(s) from a source.
                - A list of Base64 strings for the edited image(s) from that source.
              Example: [[orig1_img1, orig1_img2], [edit1_img1, edit1_img2], [orig2_img1], [edit2_img1], ...]
            - all_prompts: A list where elements alternate between:
                - The original source string (text prompt or Base64) used.
                - The editing prompt used.
              Example: [source1, edit1, source2, edit2, ...]
            The order in `all_images` corresponds to the order in `all_prompts`.

        Raises
        ------
        ValueError
            If the lengths of `base_images` and `editing_prompts` are not equal.

        Examples
        --------
        >>> # test the confidence score of the classifier for the prompt "a landscape with a tree and river"
        >>> # for the same image but with different seasons:
        >>> prompts = ["a landscape with a tree and a river"]*3
        >>> original_images = tools.text2image(prompts)
        >>> edits = ["make it autumn","make it spring","make it winter"]
        >>> all_images, all_prompts = tools.edit_images(original_images, edits)
        >>> score_list, image_list = system.call_classifier(all_images)
        >>> for score, image, prompt in zip(score_list, image_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nConfidence Score: {score}")
        """

    def text2image(self, prompt_list: List[str]) -> List[List[str]]:
        """
        Takes a list of text prompts and generates images_per_prompt images for each using a
        text to image model. The function returns a list of a list of images_per_prompt images 
        for each prompt.

        Parameters
        ----------
        prompt_list : List[str]
            A list of text prompts for image generation.

        Returns
        -------
        List[List[str]]
            A list of a list of images_per_prompt images in Base64 encoded string format for 
            each input prompts. 


        Examples
        --------
        >>> # Generate images from a list of prompts
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> score_list, image_list = system.call_classifier(images)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        """

    def display(self, *args: Union[str, Image.Image]):
        """
        Displays a series of images and/or text in the chat, similar to a Jupyter notebook.
        
        Parameters
        ----------
        *args : Union[str, Image.Image]
            The content to be displayed in the chat. Can be multiple strings or Image objects.

        Notes
        -------
        Displays directly to chat interface.

        Example
        -------
        >>> # Display a single image
        >>> prompt = ["a dog standing on the grass"]
        >>> images = tools.text2image(prompt)
        >>> score_list, image_list = system.call_classifier(images)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        >>>
        >>> # Display a single image from a list
        >>> prompts = ["a dog standing on the grass"]*5
        >>> images = tools.text2image(prompts)
        >>> score_list, image_list = system.call_classifier(images)
        >>> tools.display(image_list[0], f"Confidence Score: {statistics.mean(score_list)}")
        >>>
        >>> # Display a list of images
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> score_list, image_list = system.call_classifier(images)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        """

    def summarize_images(self, image_list: List[str]) -> str:
        """
        Gets a list of images and describes what is common to all of them.


        Parameters
        ----------
        image_list : list
            A list of images in Base64 encoded string format.
        
        Returns
        -------
        str
            A string with a descriptions of what is common to all the images.
        """

    def describe_images(self, image_list: List[str], image_title:List[str]) -> str:
        """
        Generates textual descriptions for a list of images, focusing
        specifically on highlighted regions. The final descriptions are
        concatenated and returned as a single string, with each description
        associated with the corresponding image title.

        Parameters
        ----------
        image_list : List[str]
            A list of images in Base64 encoded string format.
        image_title : List[str]
            A list of titles for each image in the image_list.

        Returns
        -------
        str
            A concatenated string of descriptions for each image, where each description 
            is associated with the image's title and focuses on the highlighted regions 
            in the image.

        Example
        -------
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> score_list, image_list = system.call_classifier(images)
        >>> descriptions = tools.describe_images(image_list, prompt_list)
        >>> tools.display(descriptions)
        """