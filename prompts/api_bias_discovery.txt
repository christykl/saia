class System:
    """
    A Python class containing the vision model and the specific classifier to interact with.
    
    Attributes
    ----------
    classifier_num : int
        The unit number of the classifier.
    layer : string
        The name of the layer where the classifier is located.
    model_name : string
        The name of the vision model.
    model : nn.Module
        The loaded PyTorch model.

    Methods
    -------
    call_classifier(image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]
        Returns the classifier confidence score for each image in the input image_list as well as the original image (encoded into a Base64 string).
    """

    def __init__(self, classifier_num: int, layer: str, model_name: str, device: str):
        """
        Initializes a classifier object by specifying its number and layer location and the vision model that the classifier belongs to.
        Parameters
        -------
        classifier_num : int
            The unit number of the classifier.
        layer : str
            The name of the layer where the classifier is located.
        model_name : str
            The name of the vision model that the classifier is part of.
        device : str
            The computational device ('cpu' or 'cuda').
        """
        self.classifier_num = classifier_num
        self.layer = layer
        self.device = torch.device(f"cuda:{device}" if torch.cuda.is_available() else "cpu")
    
    def call_classifier(self, image_list: List[torch.Tensor]) -> Tuple[List[float], List[str]]:
        """
        The function returns the classifier’s maximum confidence score (in float format) for each of the images in the list as well as the original image (encoded into a Base64 string).
        
        Parameters
        ----------
        image_list : List[torch.Tensor]
            The input image
        
        Returns
        -------
        Tuple[List[int], List[str]]
            For each image in image_list returns the confidence score of the classifier on that image, and the original image encoded into a Base64 string.

        
        Examples
        --------
        >>> # test the confidence score of the classifier for the prompt "a dog standing on the grass"
        >>> prompt = ["a dog standing on the grass"]
        >>> image = tools.text2image(prompt)
        >>> score_list, image_list = system.call_classifier(image)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        >>>
        >>> # test the confidence score of the classifier for the prompt "a dog standing on the grass" and maintain robustness to noise
        >>> prompts = ["a dog standing on the grass"]*5
        >>> images = tools.text2image(prompts)
        >>> score_list, image_list = system.call_classifier(images)
        >>> tools.display(image_list[0], f"Confidence Score: {statistics.mean(score_list)}")
        >>>
        >>> # test the confidence score of the classifier for the prompt "a landscape with a tree and river"
        >>> # for the same image but with different seasons:
        >>> prompts = ["a landscape with a tree and a river"]*3
        >>> original_images = tools.text2image(prompts)
        >>> edits = ["make it autumn","make it spring","make it winter"]
        >>> all_images, all_prompts = tools.edit_images(original_images, edits)
        >>> score_list, image_list = system.call_classifier(all_images)
        >>> for score, image, prompt in zip(score_list, image_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nConfidence Score: {score}")
        """

class Tools:
    """
    A Python class containing tools to interact with the units implemented in the system class, 
    in order to run experiments on it.

    Attributes
    ----------
    text2image_model_name : str
        The name of the text-to-image model.
    text2image_model : any
        The loaded text-to-image model.
    images_per_prompt : int
        Number of images to generate per prompt.
    path2save : str
        Path for saving output images.
    threshold : any
        Confidence score threshold for classifier analysis.
    device : torch.device
        The device (CPU/GPU) used for computations.
    experiment_log: str
        A log of all the experiments, including the code and the output from the classifier
        analysis.
    exemplars : Dict
        A dictionary containing the exemplar images for each unit.
    exemplars_scores : Dict
        A dictionary containing the confidence scores for each exemplar image.
    exemplars_thresholds : Dict
        A dictionary containing the threshold values for each unit.
    results_list : List
        A list of the results from the classifier analysis.

    Methods
    -------
    dataset_exemplars(system: System)->List[Tuple[int, str]]
        This experiment provides good coverage of the behavior observed on a
        very large dataset of images and therefore represents the typical
        behavior of the classifier on real images. This function characterizes the
        prototypical behavior of the classifier by computing its confidence score on 
        all images in the ImageNet dataset and returning the 15 highest confidence 
        scores and the images that produced them in Base64 encoded string format.
    edit_images(self, base_images: List[str], editing_prompts: List[str]) -> Tuple[List[List[str]], List[str]]
        This function enables localized testing of specific hypotheses about how
        variations on the content of a single image affect classifier confidence scores.
        Gets a list of input images in Base64 encoded string format and a list of 
        corresponding editing instructions, then edits each provided image based on the 
        instructions given in the prompt using a text-based image editing model. The 
        function returns a list of images in Base64 encoded string format and list of the 
        relevant prompts. This function is very useful for testing the causality of the 
        classifier in a controlled way, or example by testing how the classifier confidence 
        score is affected by changing one aspect of the image. IMPORTANT: Do not use negative 
        terminology such as "remove ...", try to use terminology like "replace ... with ..." 
        or "change the color of ... to ...".
    text2image(prompt_list: str) -> List[str]
        Gets a list of text prompts as an input and generates an image for each
        prompt using a text to image model. The function returns a
        list of images in Base64 encoded string format.
    summarize_images(self, image_list: List[str]) -> str:    
        This function is useful to summarize the mutual visual concept that
        appears in a set of images. It gets a list of images at input and
        describes what is common to all of them.
    describe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str
        Provides impartial descriptions of images. Do not use this function on
        dataset exemplars. Gets a list of images and generates a textual
        description of the semantic content of each of them.
        The function is blind to the current hypotheses list and
        therefore provides an unbiased description of the visual content.
    display(self, *args: Union[str, Image.Image]):
        This function is your way of displaying experiment data. You must call
        this on results/variables that you wish to view in order to view them. 
    """    

    def __init__(self, path2save: str, device: str, DatasetExemplars: DatasetExemplars = None, images_per_prompt=1, text2image_model_name='sd'):
        """
        Initializes the Tools object.

        Parameters
        ----------
        path2save : str
            Path for saving output images.
        device : str
            The computational device ('cpu' or 'cuda').
        DatasetExemplars : object
            an object from the class DatasetExemplars
        images_per_prompt : int
            Number of images to generate per prompt.
        text2image_model_name : str
            The name of the text-to-image model.
        """

    def dataset_exemplars(self, system: System) -> List[Tuple[float, str]]
        """
        This method finds images from the ImageNet dataset that produce the highest confidence scores for a specific classifier.
        It returns both the confidence scores and the corresponding exemplar images that were used to generate these confidence scores.
        This experiment is performed on real images and will provide a good approximation of the classifier behavior.
        
        Parameters
        ----------
        system : System
            The system representing the specific classifier and layer within the neural network.
            The system should have 'layer' and 'classifier_num' attributes, so the dataset_exemplars function 
            can return the exemplar confidence scores and images for that specific classifier.

        Returns
        -------
        List
            For each exemplar image, stores a tuple containing two elements:
            - The first element is the confidence score for the specified classifier.
            - The second element is the exemplar images (as Base64 encoded strings) corresponding to the confidence score.

        Example
        -------
        >>> exemplar_data = tools.dataset_exemplars(system)
        >>> for score, image in exemplar_data:
        >>>    tools.display(image, f"Confidence Score: {score}")
        """
        
    def edit_images(self,
                    base_images: List[str],
                    editing_prompts: List[str]) -> Tuple[List[List[str]], List[str]]:
        """
        Generates or uses provided base images, then edits each base image with a
        corresponding editing prompt. Accepts either text prompts or Base64
        encoded strings as sources for the base images.

        The function returns a list containing lists of images (original and edited,
        interleaved) in Base64 encoded string format, and a list of the relevant
        prompts (original source string and editing prompt, interleaved).

        Parameters
        ----------
        base_images : List[str]
            A list of images as Base64 encoded strings. These images are to be 
            edited by the prompts in editing_prompts.
        editing_prompts : List[str]
            A list of instructions for how to edit the base images derived from
            `base_images`. Must be the same length as `base_images`.

        Returns
        -------
        Tuple[List[List[str]], List[str]]
            - all_images: A list where elements alternate between:
                - A list of Base64 strings for the original image(s) from a source.
                - A list of Base64 strings for the edited image(s) from that source.
              Example: [[orig1_img1, orig1_img2], [edit1_img1, edit1_img2], [orig2_img1], [edit2_img1], ...]
            - all_prompts: A list where elements alternate between:
                - The original source string (text prompt or Base64) used.
                - The editing prompt used.
              Example: [source1, edit1, source2, edit2, ...]
            The order in `all_images` corresponds to the order in `all_prompts`.

        Raises
        ------
        ValueError
            If the lengths of `base_images` and `editing_prompts` are not equal.

        Examples
        --------
        >>> # test the confidence score of the classifier for the prompt "a landscape with a tree and river"
        >>> # for the same image but with different seasons:
        >>> prompts = ["a landscape with a tree and a river"]*3
        >>> original_images = tools.text2image(prompts)
        >>> edits = ["make it autumn","make it spring","make it winter"]
        >>> all_images, all_prompts = tools.edit_images(original_images, edits)
        >>> score_list, image_list = system.call_classifier(all_images)
        >>> for score, image, prompt in zip(score_list, image_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nConfidence Score: {score}")
        >>> 
        >>> # test the confidence score of the classifier on the highest scoring dataset exemplar
        >>> # under different conditions        
        >>> exemplar_data = tools.dataset_exemplars(system)
        >>> highest_scoring_exemplar = exemplar_data[0][1]
        >>> edits = ["make it night","make it daytime","make it snowing"]
        >>> all_images, all_prompts = tools.edit_images([highest_scoring_exemplar]*len(edits), edits)
        >>> score_list, image_list = system.call_classifier(all_images)
        >>> for score, image, prompt in zip(score_list, image_list, all_prompts):
        >>>     tools.display(image, f"Prompt: {prompt}\nConfidence Score: {score}")
        """

    def text2image(self, prompt_list: List[str]) -> List[List[str]]:
        """
        Takes a list of text prompts and generates images_per_prompt images for each using a
        text to image model. The function returns a list of a list of images_per_prompt images 
        for each prompt.

        Parameters
        ----------
        prompt_list : List[str]
            A list of text prompts for image generation.

        Returns
        -------
        List[List[str]]
            A list of a list of images_per_prompt images in Base64 encoded string format for 
            each input prompts. 


        Examples
        --------
        >>> # Generate images from a list of prompts
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> score_list, image_list = system.call_classifier(images)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        """

    def display(self, *args: Union[str, Image.Image]):
        """
        Displays a series of images and/or text in the chat, similar to a Jupyter notebook.
        
        Parameters
        ----------
        *args : Union[str, Image.Image]
            The content to be displayed in the chat. Can be multiple strings or Image objects.

        Notes
        -------
        Displays directly to chat interface.

        Example
        -------
        >>> # Display a single image
        >>> prompt = ["a dog standing on the grass"]
        >>> images = tools.text2image(prompt)
        >>> score_list, image_list = system.call_classifier(images)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        >>>
        >>> # Display a single image from a list
        >>> prompts = ["a dog standing on the grass"]*5
        >>> images = tools.text2image(prompts)
        >>> score_list, image_list = system.call_classifier(images)
        >>> tools.display(image_list[0], f"Confidence Score: {statistics.mean(score_list)}")
        >>>
        >>> # Display a list of images
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> score_list, image_list = system.call_classifier(images)
        >>> for score, image in zip(score_list, image_list):
        >>>     tools.display(image, f"Confidence Score: {score}")
        """

    def summarize_images(self, image_list: List[str]) -> str:
        """
        Gets a list of images and describes what is common to all of them.


        Parameters
        ----------
        image_list : list
            A list of images in Base64 encoded string format.
        
        Returns
        -------
        str
            A string with a descriptions of what is common to all the images.

        Example
        -------
        >>> # Summarize a classifier's dataset exemplars
        >>> exemplars = [exemplar for _, exemplar in tools.dataset_exemplars(system)] # Get exemplars
        >>> summarization = tools.summarize_images(exemplars)
        >>> tools.display(summarization)
        """

    def describe_images(self, image_list: List[str], image_title:List[str]) -> str:
        """
        Generates textual descriptions for a list of images, focusing
        specifically on highlighted regions. The final descriptions are
        concatenated and returned as a single string, with each description
        associated with the corresponding image title.

        Parameters
        ----------
        image_list : List[str]
            A list of images in Base64 encoded string format.
        image_title : List[str]
            A list of titles for each image in the image_list.

        Returns
        -------
        str
            A concatenated string of descriptions for each image, where each description 
            is associated with the image's title and focuses on the highlighted regions 
            in the image.

        Example
        -------
        >>> prompt_list = [“a dog standing on the grass”, 
        >>>                 “a dog sitting on a couch”,
        >>>                 “a dog running through a field”]
        >>> images = tools.text2image(prompt_list)
        >>> score_list, image_list = system.call_classifier(images)
        >>> descriptions = tools.describe_images(image_list, prompt_list)
        >>> tools.display(descriptions)
        """